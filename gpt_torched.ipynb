{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  sklearn import linear_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "with open ('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPLORE DATA\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE DATA\n",
    "# Most basic one-hot encoding.\n",
    "# What is a tokenizer? This is a tokenizer:\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "def encode(s: str) -> torch.tensor:\n",
    "    return torch.tensor([stoi[c] for c in s])\n",
    "def decode(s: torch.tensor) -> str:\n",
    "    return ''.join([itos[int(c)] for c in list(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 43, 50, 50, 53,  6,  1, 58, 46, 43, 56, 43])\n",
      "hello, there\n"
     ]
    }
   ],
   "source": [
    "print(encode('hello, there'))\n",
    "print(decode(encode('hello, there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google uses : SentencePiece\n",
    "# tiktoken used for gpt2 (this is what we build next time?)\n",
    "# fast BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27865/2698845693.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# SPLIT DATA\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:10])\n",
    "n = int(0.8*len(text))\n",
    "n2 = int(0.9*len(text))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:n2]\n",
    "test_data = data[n2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS FOR TRAINING\n",
    "\n",
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "\n",
    "# BATCHING\n",
    "# This is pretty similar to how batches were processed in makemore\n",
    "def get_batch(split, block_size, batch_size, device='cuda'):\n",
    "    data = train_data if split == 'train' else val_data if  split == 'val' else test_data \n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # New, adding to put data on the GPU:\n",
    "    x, y  = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# xb, yb = get_batch('train', block_size, batch_size)\n",
    "# print(xb.shape, yb.shape)\n",
    "# print(decode(xb[0]), decode(yb[0]))\n",
    "\n",
    "# ESTIMATE LOSS\n",
    "# Better estimate than just using the loss on the last batchâ€”\n",
    "# get a less noisy result by averaging over multiple batches.\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, block_size, batch_size, eval_iters,  device='cuda'):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split, block_size, batch_size, device=device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the gpt model looks like using pytorch's built-in modules:\n",
    "\n",
    "# Establishing the structure for a torch model by revisiting bigram\n",
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=n_embed, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.layernorm = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: targets are (B, T)\n",
    "\n",
    "\n",
    "        # Embedding idx into the network:\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, n_embed)o\n",
    "        pos_indices = torch.arange(T, device=idx.device)\n",
    "        pos_emb = self.position_embedding_table(pos_indices) # (T, n_embed)\n",
    "        \n",
    "        # Passing idx through layers:\n",
    "        x = token_emb + pos_emb # (B, T, n_embed)\n",
    "        # TransformerEncoder expects (T, B, n_embed) instead of (B, T, n_embed):\n",
    "        # x = x.transpose(0,1)\n",
    "        # TransformerEncoder doesn't mask for only future values (\"causal masking\"), so make a mask:\n",
    "        causal_mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n",
    "        x = self.transformer_encoder(x, mask=causal_mask)\n",
    "        # Un-reverse the T, B swap for final layers:\n",
    "        # x = x.transpose(0, 1)\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        \n",
    "        if targets is not None: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens in case the context window\n",
    "            # is too long for the model to handle\n",
    "            # (note this means the model can only accept context up to block_size)\n",
    "            # (in length for its input)\n",
    "            idx_cond = idx[:, -self.block_size:] # (B, T)\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # Becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "block_size = 256  #T,  = context window? If context window is longer, need to truncate context window for transformer to understand how to predict\n",
    "batch_size = 64 #B,  how many blocks to process at once\n",
    "vocab_size = 65 # the first C, in Karpathy's shorthand\n",
    "learning_rate = 3e-4\n",
    "train_steps = 5000\n",
    "device = 'cuda'\n",
    "eval_iters = 40\n",
    "eval_interval = 500\n",
    "n_embed = 384 # the second C, in Karpathy's shorthand\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "dropout = 0.2 # drop 0.2 of layers\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('**\\n**\\n**\\n**ERROR: CUDA ISNT RUNNING YET. CODE BELOW WILL FAIL**\\n**\\n**\\n**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device=device)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we use Torch to train?\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# A more advanced and modern training method:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Train Loss 4.26886510848999, Val Loss 4.27038049697876\n",
      "Step 500, Train Loss 1.8219738006591797, Val Loss 1.9243099689483643\n",
      "Step 1000, Train Loss 1.4936542510986328, Val Loss 1.6345638036727905\n",
      "Step 1500, Train Loss 1.3586989641189575, Val Loss 1.5396641492843628\n",
      "Step 2000, Train Loss 1.2758090496063232, Val Loss 1.487217664718628\n",
      "Step 2500, Train Loss 1.2222506999969482, Val Loss 1.4613816738128662\n",
      "Step 3000, Train Loss 1.1734118461608887, Val Loss 1.4511969089508057\n",
      "Step 3500, Train Loss 1.135192632675171, Val Loss 1.439039707183838\n",
      "Step 4000, Train Loss 1.0961920022964478, Val Loss 1.4240528345108032\n",
      "Step 4500, Train Loss 1.0570170879364014, Val Loss 1.4364012479782104\n",
      "Step 5000, Train Loss 1.0289815664291382, Val Loss 1.4433239698410034\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "for steps in range(train_steps+1):\n",
    "    xb, yb = get_batch('train', block_size, batch_size, device=device)\n",
    "\n",
    "    # Evaluate loss every eval_interval steps\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss(model, block_size, batch_size, eval_iters, device=device)\n",
    "        print(f'Step {steps}, Train Loss {losses[\"train\"]}, Val Loss {losses[\"val\"]}')\n",
    " \n",
    "    # Backpropogate\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE MODEL\n",
    "# torch.save(model.state_dict(), \"gpt_torched_pretrained.pth\")\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL ONCE SAVED\n",
    "model = LanguageModel(vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device=device)\n",
    "model.load_state_dict(torch.load(\"gpt_torched_pretrained.pth\", map_location=device))\n",
    "model = model.to('cuda')\n",
    "print(model.device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ISABELLA:\n",
      "No do for my lad, I see with me; I pray you.\n",
      "\n",
      "LUCIO:\n",
      "I could buy you know speak! Montague,\n",
      "My lady faces and half of their own brains\n",
      "Peter blazen out me and speed in sheapen\n",
      "My tears o' the boy's men: like a love-man's,\n",
      "In the kindy something man Death, Valia.\n",
      "Madam, in marriagech, let image away.\n",
      "\n",
      "LARTIUS:\n",
      "What come there? what's the horse cannow?\n",
      "\n",
      "All:\n",
      "Post-hail, by my royal eyescapes,\n",
      "Which, fearing: defexanting is anothing at ours.\n",
      "\n",
      "MENENIUS:\n",
      "Comme, ye ask, and well fair lovers' \n"
     ]
    }
   ],
   "source": [
    "# GENERATE TEXT\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context,500)[0])) # (slightly different because I encode/decode in torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33396887) # deeznuts\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myanacondaenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
