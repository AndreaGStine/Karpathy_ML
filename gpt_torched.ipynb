{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  sklearn import linear_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "with open ('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXPLORE DATA\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE DATA\n",
    "# Most basic one-hot encoding.\n",
    "# What is a tokenizer? This is a tokenizer:\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "def encode(s: str) -> torch.tensor:\n",
    "    return torch.tensor([stoi[c] for c in s])\n",
    "def decode(s: torch.tensor) -> str:\n",
    "    return ''.join([itos[int(c)] for c in list(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 43, 50, 50, 53,  6,  1, 58, 46, 43, 56, 43])\n",
      "hello, there\n"
     ]
    }
   ],
   "source": [
    "print(encode('hello, there'))\n",
    "print(decode(encode('hello, there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google uses : SentencePiece\n",
    "# tiktoken used for gpt2 (this is what we build next time?)\n",
    "# fast BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54438/2698845693.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# SPLIT DATA\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:10])\n",
    "n = int(0.8*len(text))\n",
    "n2 = int(0.9*len(text))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:n2]\n",
    "test_data = data[n2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS FOR TRAINING\n",
    "\n",
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "\n",
    "# BATCHING\n",
    "# This is pretty similar to how batches were processed in makemore\n",
    "def get_batch(split, block_size, batch_size, device='cuda'):\n",
    "    data = train_data if split == 'train' else val_data if  split == 'val' else test_data \n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # New, adding to put data on the GPU:\n",
    "    x, y  = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# xb, yb = get_batch('train', block_size, batch_size)\n",
    "# print(xb.shape, yb.shape)\n",
    "# print(decode(xb[0]), decode(yb[0]))\n",
    "\n",
    "# ESTIMATE LOSS\n",
    "# Better estimate than just using the loss on the last batch—\n",
    "# get a less noisy result by averaging over multiple batches.\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, block_size, batch_size, eval_iters,  device='cuda'):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split, block_size, batch_size, device=device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT MODEL\n",
    "\n",
    "# What a head of attention looks like, all put together and torchified:\n",
    "\n",
    "\n",
    "# Just a linear layer followed by Gelu / Relu / etc.\n",
    "# Why add a feedforward layer after the attention block?\n",
    "# Because attention is what allows each token to \"see\" each other\n",
    "# But the tokens also need to \"think about what they saw\"\n",
    "# The feedfoward layer adds some neurons for the thinking to happen\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4*n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embed, n_embed), # Proj layer at the end as with MHA above\n",
    "            # Added for skip connection. \n",
    "            # Then we multiplied the middle layer by 4. Why? Because the paper did.\n",
    "            # Idea: Attention slow anyway, so it won't slow down computation to\n",
    "            # make this feedforward wider ~ more expressive. If it doesn't hurt,\n",
    "            # then why not in case it helps?\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# A transformer block, albeit sans the encoder-decoder cross-attention mechanism:\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, block_size, n_embed, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.sa = nn.MultiHeadAttention(embed_dim=n_embed, num_heads=num_heads, dropout=dropout, batch_first=False)\n",
    "        self.ffwd = FeedForward(n_embed, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed) # To be more Principled(tm), let's use \n",
    "        self.ln2 = nn.LayerNorm(n_embed) # our own LayerNorm, unlike Karpathy!\n",
    "    \n",
    "    # Before skip connection:\n",
    "    # def forward(self, x):\n",
    "    #     x = self.sa(x)\n",
    "    #     x = self.ffwd(x)\n",
    "    #     return x\n",
    "    # Now with skip connection (big change, I know):\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Establishing the structure for a torch model by revisiting bigram\n",
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "        # Bigram: Just token embedding table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # New: linear layer. Why? It's temporary—we'll expand it into more later\n",
    "        # We also changed the embedding table to be n_embed vs vocab_size\n",
    "        # Oh, I get it. Before our embedding table was just a static\n",
    "        # pairing of any two characters, as in the bigram model,\n",
    "        # But now we're creating 32 dimensions of \"meaning\" for our 65 characters\n",
    "        # Learned and encoded with a simple linear layer\n",
    "\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[nn.Transformer(d_model=vocab_size*num_heads, nhead=num_heads, num_encoder_layers=n_embed, dropout=dropout, device=device) for _ in range(num_layers)],\n",
    "            nn.LayerNorm(n_embed),\n",
    "            nn.Linear(n_embed, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx, targets are (B, T)\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # (T, n_embed)\n",
    "        x = token_emb + pos_emb # (B, T, n_embed)\n",
    "        logits = self.blocks(x)\n",
    "\n",
    "        \n",
    "        if targets is not None: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens in case the context window\n",
    "            # is too long for the model to handle\n",
    "            # (note this means the model can only accept context up to block_size)\n",
    "            # (in length for its input)\n",
    "            idx_cond = idx[:, -self.block_size:] # (B, T)\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # Becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "**\n",
      "**\n",
      "**ERROR: CUDA ISNT RUNNING YET. CODE BELOW WILL FAIL**\n",
      "**\n",
      "**\n",
      "**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agscoding/anaconda3/envs/myanacondaenvironment/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "block_size = 256  #T,  = context window? If context window is longer, need to truncate context window for transformer to understand how to predict\n",
    "batch_size = 64 #B,  how many blocks to process at once\n",
    "vocab_size = 65 # the first C, in Karpathy's shorthand\n",
    "learning_rate = 3e-4\n",
    "train_steps = 5000\n",
    "device = 'cuda'\n",
    "eval_iters = 40\n",
    "eval_interval = 500\n",
    "n_embed = 384 # the second C, in Karpathy's shorthand\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "dropout = 0.2 # drop 0.2 of layers\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('**\\n**\\n**\\n**ERROR: CUDA ISNT RUNNING YET. CODE BELOW WILL FAIL**\\n**\\n**\\n**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[11], line 75\u001b[0m, in \u001b[0;36mLanguageModel.__init__\u001b[0;34m(self, vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# New: linear layer. Why? It's temporary—we'll expand it into more later\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# We also changed the embedding table to be n_embed vs vocab_size\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Oh, I get it. Before our embedding table was just a static\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# pairing of any two characters, as in the bigram model,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# But now we're creating 32 dimensions of \"meaning\" for our 65 characters\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Learned and encoded with a simple linear layer\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(block_size, n_embed)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;241m*\u001b[39m[nn\u001b[38;5;241m.\u001b[39mTransformer(d_model\u001b[38;5;241m=\u001b[39mvocab_size\u001b[38;5;241m*\u001b[39mnum_heads, nhead\u001b[38;5;241m=\u001b[39mnum_heads, num_encoder_layers\u001b[38;5;241m=\u001b[39mn_embed, dropout\u001b[38;5;241m=\u001b[39mdropout, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)],\n\u001b[1;32m     76\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLayerNorm(n_embed),\n\u001b[1;32m     77\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(n_embed, vocab_size)\n\u001b[1;32m     78\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# New: linear layer. Why? It's temporary—we'll expand it into more later\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# We also changed the embedding table to be n_embed vs vocab_size\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Oh, I get it. Before our embedding table was just a static\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# pairing of any two characters, as in the bigram model,\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# But now we're creating 32 dimensions of \"meaning\" for our 65 characters\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Learned and encoded with a simple linear layer\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(block_size, n_embed)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;241m*\u001b[39m[nn\u001b[38;5;241m.\u001b[39mTransformer(d_model\u001b[38;5;241m=\u001b[39mvocab_size\u001b[38;5;241m*\u001b[39mnum_heads, nhead\u001b[38;5;241m=\u001b[39mnum_heads, num_encoder_layers\u001b[38;5;241m=\u001b[39mn_embed, dropout\u001b[38;5;241m=\u001b[39mdropout, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)],\n\u001b[1;32m     76\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLayerNorm(n_embed),\n\u001b[1;32m     77\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(n_embed, vocab_size)\n\u001b[1;32m     78\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myanacondaenvironment/lib/python3.11/site-packages/torch/nn/modules/transformer.py:125\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, activation, custom_encoder, custom_decoder, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m custom_encoder\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     encoder_layer \u001b[38;5;241m=\u001b[39m TransformerEncoderLayer(\n\u001b[1;32m    126\u001b[0m         d_model,\n\u001b[1;32m    127\u001b[0m         nhead,\n\u001b[1;32m    128\u001b[0m         dim_feedforward,\n\u001b[1;32m    129\u001b[0m         dropout,\n\u001b[1;32m    130\u001b[0m         activation,\n\u001b[1;32m    131\u001b[0m         layer_norm_eps,\n\u001b[1;32m    132\u001b[0m         batch_first,\n\u001b[1;32m    133\u001b[0m         norm_first,\n\u001b[1;32m    134\u001b[0m         bias,\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    137\u001b[0m     encoder_norm \u001b[38;5;241m=\u001b[39m LayerNorm(\n\u001b[1;32m    138\u001b[0m         d_model, eps\u001b[38;5;241m=\u001b[39mlayer_norm_eps, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m TransformerEncoder(\n\u001b[1;32m    141\u001b[0m         encoder_layer, num_encoder_layers, encoder_norm\n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myanacondaenvironment/lib/python3.11/site-packages/torch/nn/modules/transformer.py:728\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    726\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype}\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 728\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m MultiheadAttention(\n\u001b[1;32m    729\u001b[0m     d_model,\n\u001b[1;32m    730\u001b[0m     nhead,\n\u001b[1;32m    731\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m    732\u001b[0m     bias\u001b[38;5;241m=\u001b[39mbias,\n\u001b[1;32m    733\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39mbatch_first,\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[1;32m    735\u001b[0m )\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myanacondaenvironment/lib/python3.11/site-packages/torch/nn/modules/activation.py:1092\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_proj_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m-> 1092\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[1;32m   1093\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_proj_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk_proj_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myanacondaenvironment/lib/python3.11/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "model = LanguageModel(vocab_size, n_embed, block_size, num_heads, num_layers, dropout, device=device)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we use Torch to train?\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "# A more advanced and modern training method:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Train Loss 4.310555458068848, Val Loss 4.308139801025391\n",
      "Step 2000, Train Loss 1.2872371673583984, Val Loss 1.504728078842163\n",
      "Step 4000, Train Loss 1.0742766857147217, Val Loss 1.4452369213104248\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "for steps in range(train_steps+1):\n",
    "    xb, yb = get_batch('train', block_size, batch_size, device=device)\n",
    "\n",
    "    # Evaluate loss every eval_interval steps\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss(model, block_size, batch_size, eval_iters, device=device)\n",
    "        print(f'Step {steps}, Train Loss {losses[\"train\"]}, Val Loss {losses[\"val\"]}')\n",
    " \n",
    "    # Backpropogate\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But that shame I was born to thee, here's not with\n",
      "Nothing in our liCn and terms actor. King bo my little,\n",
      "for I'll blood thee bite my name\n",
      "That with one thing enjoy. I have been mine.\n",
      "Henry, the soldier slain, my liege's fear!\n",
      "Am I know, term this foolish my dream. I\n",
      "Besiece lay to be jointed to after thee,\n",
      "I'll talk our terrible wrong, and till hole content\n",
      "Thime such an unavoidest many to hear\n",
      "Bissole themselvess to him: though I know\n",
      "And see, and spirit him, my matters and province\n",
      "ISlay to \n"
     ]
    }
   ],
   "source": [
    "# GENERATE TEXT\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context,500)[0])) # (slightly different because I encode/decode in torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(33396887) # deeznuts\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7356, 0.2644, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3887, 0.6100, 0.0013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1946, 0.0375, 0.6218, 0.1460, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1335, 0.3659, 0.0400, 0.4292, 0.0315, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1937, 0.2104, 0.0658, 0.5013, 0.0088, 0.0200, 0.0000, 0.0000],\n",
      "        [0.0131, 0.0401, 0.6925, 0.0335, 0.0112, 0.1395, 0.0699, 0.0000],\n",
      "        [0.0499, 0.0443, 0.0013, 0.6424, 0.1025, 0.0480, 0.0999, 0.0118]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# THE IDEA OF ATTENTION\n",
    "# (terminology reminder for later: elements of x are \"tokens\")\n",
    "# The basic bag of words setup:\n",
    "B = 4 # batch_size\n",
    "T = 8 # block_size\n",
    "C = 32 # vocab_size\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# VERSION 1: Construct a \"bag of words\" for each letter\n",
    "# Bag of Words - averaging over the words/characters/tokens seen in x\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0) # avg over time, (C)\n",
    "\n",
    "# VERSION 2: With matrix multiplication\n",
    "# Now to do this more efficiently with matrix multiplication:\n",
    "# Lower triangular -> A row is averaged over values seen so far\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) \n",
    "# Creates batched dimension: (B, T, T) @ (B, T, C)\n",
    "# Essentially a copy of the (T, T) tensor for each b in B\n",
    "# Then multiplies (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "# print(wei)\n",
    "\n",
    "# VERSION 3: With softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "print(torch.allclose(xbow, xbow2), torch.allclose(xbow, xbow3))\n",
    "\n",
    "# print(wei)\n",
    "# print(xbow[0], xbow2[0], xbow3[0])\n",
    "\n",
    "# So what's the idea of self-attention?\n",
    "# You weight the value of each prior letter's effect on \n",
    "# the current letter. At first, it's an average\n",
    "# The weights are then learned. \n",
    "# How do you weight the value of each prior letter's effect?\n",
    "# Create a vector of weights of all prior letters, for the \n",
    "# current letter. \n",
    "\n",
    "# VERSION 4: Where weights are learned\n",
    "# Before, wei established as all uniform\n",
    "# Don't want all uniform - want certain tokens to find\n",
    "# other tokens more interesting. ex: Vowel should find consonant more likely\n",
    "# \"curiosity\" should find \"killed the cat\" more interesting\n",
    "# (when we get to the tokenizer)\n",
    "# Interestingness of other tokens should be data-dependent:\n",
    "# what a token finds interesting depends on the block and the token\n",
    "# But want a data-independent method of gathering information from the past,\n",
    "# where the data can be \"plugged in\" at a given time\n",
    "# Attention solves this problem as follows:\n",
    "# Every token emits 2 vectors: Query, key\n",
    "# Query: What am I looking for?\n",
    "# Key: What do I contain?\n",
    "# Affinities between tokens: dot product of query and key\n",
    "# If key and query align, product is larger and emits stronger signal\n",
    "# Also create Value: Aggregation vector to make output dim of a token = head_size\n",
    "# Value: What do I communicate to you?\n",
    "\n",
    "# One head of self-attention:\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "# Copying what's from before\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "print(wei[0])\n",
    "\n",
    "v = value(x) # (B, T, head_size)\n",
    "out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "\n",
    "# x: Private \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION:\n",
    "# Attention Is All You Need\n",
    "# Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin\n",
    "# https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "# 1. Attention is a communication mechanism: \n",
    "# Can think as a directed graph where nodes are tokens, edges are affinities\n",
    "# Every node has a vector of information, and it can aggregate information\n",
    "# from a weighted sum of of nodes pointing to it's information\n",
    "# Now which nodes are pointed to?\n",
    "# Every node points to itself\n",
    "# Node i points to every node j > i\n",
    "# This is the appropriate graph structure for autoregressive data like language\n",
    "# (or stonks?)\n",
    "# In general, you can apply attention to any directed graph\n",
    "# But to encode the graph structure, you'd need to modify tril appropriately\n",
    "# (so for example, if I had image data, a node would point to all nearby nodes)\n",
    "# (rather than just the nodes that come after it)\n",
    "\n",
    "# 2. Attention has no notion of space (unlike with conv):\n",
    "# Attention acts on a set of vectors in the directed graph\n",
    "# Nodes don't know where they are in space\n",
    "# Need to add the positional information in.\n",
    "# This is what was done in the model above with the lines: \n",
    "# self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "# pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embed)\n",
    "\n",
    "# 3. There is no communication across batch dimension:\n",
    "# Each item in the batch is processed independently; \n",
    "# no communication across B axis\n",
    "\n",
    "#  4. How to modify tril?\n",
    "# Tril is chosen because only what has already been said can inform\n",
    "# the next thing I say, rather than what I will say later informing what\n",
    "# I currently say (we don't plan our sentences out in advance unless giving a speech)\n",
    "# Many cases: Want all nodes to talk to each other fully\n",
    "# E.g.: Sentiment analysis of a sentence - future words should still\n",
    "# inform the current word.\n",
    "# Encoder blocks: All nodes talk to all other nodes, because future words\n",
    "# inform the meaning of the present word\n",
    "# Decoder block: Nodes only talk to nodes that come before them, \n",
    "# because we don't know the future when we're generating text\n",
    "\n",
    "# 5. Self-attention vs cross-attention:\n",
    "# Self-attention: The same source x is used for key, query, value\n",
    "# Encoder-decoder transformers might have:\n",
    "# Queries: x\n",
    "# Keys and values: y\n",
    "# Where y is some other source of (side?) information\n",
    "\n",
    "# 6. Scaled attention:\n",
    "# In the \"Attention is all you need\" paper, introduce\n",
    "# \"Scaled Dot-Product Attention\":\n",
    "# Affinities are scaled by sqrt(d_k), where d_k is head_size\n",
    "# This is a necessary normalization to prevent the dot product from exploding\n",
    "# Since our linear layers are initialized with random weights,\n",
    "# their variance scales with head_size, so we need to scale it down\n",
    "# Too high variance when put thru softmax turns it into one-hot encoding\n",
    "# and we lose other relevant weights\n",
    "\n",
    "# 7. A change since \"Attention is All You Need\":\n",
    "# In the OG, Add&Norm happened *after* attention blocks\n",
    "# We do \"pre-norm\" formulation—do it before attention blocks\n",
    "\n",
    "# 8. Another note I learned from autocomplete:\n",
    "# Attention is a generalization of convolutions:\n",
    "# \"Convolution is a special case of attention where the weights are shared\n",
    "# across the input\"\n",
    "# Multi-headed self-attention -> Group convolution (not algebraic group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrary to their claim, \"Attention is All You Need\", \n",
    "# Attention is not all you need. Other papers:\n",
    "\n",
    "# SKIP CONNECTIONS / RESIDUAL CONNECTIONS: \n",
    "# Deep Residual Learning for Image Recognition\n",
    "# He, Zhang, Ren, Sun (Kaiming He? That name sounds familiar...)\n",
    "# https://arxiv.org/pdf/1512.03385\n",
    "# Reminds me of \"Kaiming Initialization\", in the makemore.ipynb!\n",
    "# Wow, same crew of 4 people! These guys must be good collaborators.\n",
    "# Why skip connections?\n",
    "# To make neural net better, add more layers 5head\n",
    "# But eventually adding more layers stops making it better. Why? \n",
    "# Because it's \"too deep\" — old layers are totally forgotten about\n",
    "# by the time you get through the pipeline. It'd be so efficient if\n",
    "# new layers could see *all* old layers instead of just the last one, right?\n",
    "# Introduce: Skip connection—let older talk to newer ones directly\n",
    "# and skip over any intermediate layers, as well as letting intermediate\n",
    "# layers talk.\n",
    "\n",
    "# What is a skip connection in more detail?\n",
    "# Say you have A -> B -> C and you want A to talk to C\n",
    "# How? Take output of B, feed that into C, and add output of A to it.\n",
    "# Simple as that.\n",
    "# Why it works is more interesting: It's because of backprop.\n",
    "# The model gets to *learn* how much of each of A and B it is that C should accept\n",
    "\n",
    "# How is this implemented? See above for the final product. What changed?\n",
    "# - Changed x = layer(x) to x += layer(x)\n",
    "# - Added to each layer (MultiHeadAttention): \n",
    "# self.proj = nn.Linear(n_embed, n_embed)\n",
    "# out = self.proj(out)\n",
    "# That way how much the layer is *used* (relative to the direct connection)\n",
    "# can be learned by the model.\n",
    "\n",
    "# LAYER NORM:\n",
    "# Layer Normalization\n",
    "# Ba, Kiros, Hinton\n",
    "# https://arxiv.org/abs/1607.06450\n",
    "# Similar to BatchNorm in goal: Every neuron should be standard normal\n",
    "# But rather than normalize a whole batch, instead just normalize\n",
    "# an individual token. How do you know you're normalizing each token\n",
    "# appropriately? By using the same normalization parameters on every token,\n",
    "# which are learned by the nn\n",
    "\n",
    "# DROPOUT\n",
    "# Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n",
    "# Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov\n",
    "# https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "# Wow, this Hinton guy seems like a big deal if he's in multiple\n",
    "# of these papers. Lemme check out his CV... Wowza! How impressive. \n",
    "# Randomly shut off some neurons every forward & backward pass\n",
    "# Regularizes the model \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT STEP: Create an encoder for the full encoder-decoder model\n",
    "# The mask that makes the attention block auto-regressive is \n",
    "# what makes it a good decoder block—it figures out to say next by what's been said\n",
    "# But what if we want to condition the attention block on extra information?\n",
    "# For example, on some other input, maybe in another language (e.g. Fr->Eng)?\n",
    "# Then you need to add an encoder that feeds into the transformer \n",
    "# With that done, we will have the entirety of a Generative, Pretrained, Transformer\n",
    "\n",
    "# What is ChatGPT versus the output of this?\n",
    "# \"Alignment\" is the only difference.\n",
    "# How does \"Alignment\" happen? \n",
    "# Reinforcement Learning. Specifically, Proximal Policy Optimization\n",
    "# Step 1: Fine-tune on human-curated data - starting policy\n",
    "# Step 2: Have raters rate responses - reward signal is learned from good responses\n",
    "# Step 3: Use PPO to train the agent how to best respond - train RL\n",
    "# Proximal Policy Optimization Algorithms\n",
    "# Schulman, Wolski, Dhariwal, Radford, Klimov\n",
    "# https://arxiv.org/pdf/1707.06347\n",
    "\n",
    "# See also:\n",
    "# https://arxiv.org/pdf/2005.14165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myanacondaenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
