{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e6db3c-c470-4f9b-ad3d-0ae21bf7fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0cb6f-1166-40d6-af4d-cb219d618a6a",
   "metadata": {},
   "source": [
    "Goal: Create a made-up name generator!\n",
    "\n",
    "What does it do? It predicts the most likely:\n",
    "- starting character\n",
    "- ending character\n",
    "- next character in sequence\n",
    "to predict new names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226b555-3eeb-4d8a-8919-8c9cf63e4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd200757-e9f9-4abf-b966-41fde5de56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d67dd5-73f0-4bba-b46b-46ef76bc971a",
   "metadata": {},
   "source": [
    " ## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627a13b-a25d-446f-9b93-ce25d8776c27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36952958-2573-4f53-9f67-6b573f7994ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a94e9-c2b2-4a0c-ac64-27b8d6f2829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e25ad9c-c260-4cfa-9d41-edc6098579d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration is so critical to this process, because it's the stage at which one comes\n",
    "# to understand the structure of the data you're working with, and *how* you can use a ML model\n",
    "# to exploit that structure.\n",
    "\n",
    "# What we're doing now: Identifying consecutive chars, starting and ending chars, which will be the\n",
    "# basic structure we identify in words for the moment.\n",
    "b = {}\n",
    "for w in words:\n",
    "    # Make a fake starting and ending character to signify which pair of letters is the start and end\n",
    "    # which we'll use later without having to treat starting and ending letters separately from\n",
    "    # consecutive letters\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b915553-c033-496d-9cd0-9c4cf9fdecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c5185-d786-4c09-8ca4-e9c23c3046ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(),key = lambda kv: -kv[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaedd0be-78a1-4a4a-bc63-0544eaadf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert this to a torch 2-tensor \n",
    "# We replace the fake starting, ending characters with a single placeholder\n",
    "# since .a means a comes first, a. means a comes last.\n",
    "N = torch.zeros((27,27), dtype=torch.float32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "stoi['.'] = 26\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3aafff-a990-461a-9e91-68fcd57aebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afa3e6-2bc6-40b8-be79-2d64b4bc4de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j,i,chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j,i,N[i,j].item(),ha=\"center\",va=\"top\",color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d9af8-aee7-4880-8382-998420479bf6",
   "metadata": {},
   "source": [
    "## A First Attempt At Prediction Using Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154c593-a574-496f-8536-002db3194395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add 1 to N in order to assign a small but nonzero probability to words that currently have 0 probability:\n",
    "P = (N+1).float()\n",
    "# This operation is an example of broadcasting:\n",
    "# Broadcasting rule: Each coordinate in the tensor must either:\n",
    "# 1. Be equal, 2. One coordinate = 1, 3. One coordinate DNE\n",
    "# The effect of the operation when a coordinate is 1: Extends coordinate to n by copying\n",
    "# So the below actually works.why resampling is done\n",
    "# The effect of the operation when a coordinate does not exist:\n",
    "# Append a new coordinate of 1 dim to the beginning of the tensor, then apply above.\n",
    "# In the below: keepdim=True turns the 27x27 tensor into a 27x1 tensor; if False, the effect\n",
    "# when broadcasting is: 27x27 -> 27 -> 1x27, so incorrrectly divides the row values by the column values.\n",
    "P /= P.sum(1, keepdims=True)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a71d1e-cc42-4d9d-ba1e-9131c9d96828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is resampling our data using a bootstrapping method. Since our samples don't have any \n",
    "# structure to their data, and we're not doing a training/testing split, this is the simplest\n",
    "# and most effective route (as opposed to k-folds)\n",
    "\n",
    "# torch.multinomial: Returns tensor where each row contains num_samples\n",
    "# indices sampled from multinomial distribution located in row of input\n",
    "# Generator: result of sampling is deterministic so it agrees with Karpathy's distribution\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(P[0], num_samples=1, replacement=True, generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed264a27-6d2f-48f8-9bc7-d83a421c482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our primitive model to print some new names.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(0,10):\n",
    "    ix = 26\n",
    "    out = []\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 26:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b649ee6-c31c-4543-89ed-e33096ac7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion to all this: Our bigram model sucks! Needs some gradient descent to be better.\n",
    "# How to evaluate quality? Determine a loss function.\n",
    "# Let's make a loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd38af14-32e1-428d-b225-4aa9e13b4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: -log likelihood function in predicting the o.g. names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e2afb-ea50-4315-9924-b64e7a3a026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f'{ch1}{ch2}: {prob:.4f} {prob:.4f}')\n",
    "print(f'{-log_likelihood/n=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df49f9-2f80-4a79-bf40-74e586272193",
   "metadata": {},
   "source": [
    "## Building a language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adea43ee-9235-4b0f-9daf-916af3f1a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set of bigrams:\n",
    "# xs: Inputs (1st char)\n",
    "# ys: Outputs (2nd char)\n",
    "xs, ys = [], []\n",
    "for w in words[:1000]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "# Note .tensor is different from .Tensor: .tensor infers datatype and creates a tensor of the\n",
    "# appropriate type, while .Tensor assumes the datatype is Float by default.\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de6c66-3c64-4478-aff1-f8eb32e7b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To multiply probabilities, it won't make sense to multiply the indices. So use \n",
    "# a one-hot encoding:\n",
    "# (by default, one_hot encodes as int vectors instead of float vectors)\n",
    "xenc = torch.nn.functional.one_hot(xs, num_classes=27).float() \n",
    "print(ys.shape)\n",
    "print(xenc.shape)\n",
    "\n",
    "W = torch.randn((27,27), requires_grad=True)\n",
    "# @ is multiplication in torch for some awful reason:\n",
    "# (xenc @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d695e82-2179-4948-924a-807019f07404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build softmax:\n",
    "def softmax(logits):\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e6a6b-f745-4f69-8d32-faebeb6743b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent:\n",
    "for k in range(0,1000):\n",
    "    # Forward Pass:\n",
    "    probs = softmax(xenc @ W)\n",
    "    # Adding W**2.mean() slightly uniformizes W: Same effect as adding 1 to the counts\n",
    "    # in the prior section:\n",
    "    loss = -probs[:,ys].log().mean() + 0.01*(W**2).mean()\n",
    "    #Backward Pass:\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -10 * W.grad\n",
    "    \n",
    "    if k % 100 == 0:\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "# for k in range(len(list(xenc[:,0]))):\n",
    "#     # forward pass:\n",
    "#     ypred = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be459335-c08d-4626-b12f-6a6a2a6ef3c7",
   "metadata": {},
   "source": [
    "# Makemore Part 2: Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "37085b04-95a9-4e3b-a8f5-b4136053ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damn My Little Pony!\n",
    "# The video for this uses a separate document, so I'm treating this as such:\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51012429-73d1-4684-84f0-2ed06adf448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the architecture of Bengio, Ducharme, Vincent, Jauvin: \n",
    "# https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "# To get i-th output, determined by: P(w_t = i | context), take as input:\n",
    "# the n-character context window, embedded as vectors through some\n",
    "# (presumably) learned embedding process for efficient and intelligent storage\n",
    "# Put these characters through an activation function (they use tanh)\n",
    "# And then softmax to obtain probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8a297b3-2467-402a-953f-ad52c3370c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "# I was putting my own spin on it  by having '.' be 26, but \n",
    "# now that we're initializing lists with it, 0 is the better encoding:\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "71978c04-cac4-48fd-b6ac-ffd6e9ba9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have new stuff:\n",
    "block_size = 3 # Context length; likely will want to adjust this\n",
    "# This is how much past stuff our MLP can see\n",
    "Xlst, Ylst = [], []\n",
    "for w in words[:5]:\n",
    "    \n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        Xlst.append(context)\n",
    "        Ylst.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(Xlst)\n",
    "Y = torch.tensor(Ylst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f8e76756-b610-4fda-bd4d-cd95f318d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Playing with tensor indexing:\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "# print(C.shape)\n",
    "# print(C[5]) # same as:\n",
    "# print(C[[5],[0,1]])\n",
    "# print(C[[5,6]])\n",
    "# print(C[[0,1]])\n",
    "# print(C[[5,6,5,6],[0,0,1,1]]) # flattens the 2x2 tensor into a 1x4 read down-right\n",
    "# print(C[[5,6],[0,1]]) # Takes the diagonal of C[5,6]\n",
    "# print(C[torch.tensor([[5,6],[0,1]])]) # Notably different from the above:\n",
    "# # Its first 2x2 matrix is that of C[[5,6]], the second is that of C[[0,1]]\n",
    "# print(torch.tensor([[5,6],[6,7],[7,8]]))\n",
    "# print(C[torch.tensor([[5,6],[6,7],[7,8]])])\n",
    "# print(torch.tensor([[5,6],[6,7],[7,8]]).shape)\n",
    "# print(C[torch.tensor([[5,6],[6,7],[7,8]])].shape)\n",
    "# # So if C is axb and M is mxn, C[M] is mxnxb: C[M][i] is an mxn matrix consisting of\n",
    "# # the ith row of M used as indices for the entries of C\n",
    "# # Hence, if M[0] = [2,3], C[2] = [4,5], C[3] = [6,7], then C[M][0] = [[4,5],[6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a7db4-d4af-44a6-a6e8-fb9c98e08e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When initializing C randomly, C[n] gives the same result as:\n",
    "# F.one_hot(torch.tensor(n), num_classes=27).float() @ C\n",
    "# So there's no need to explicitly one-hot encode the characters into C;\n",
    "# It happens \"automatically\"\n",
    "# C[X] embeds X into C\n",
    "C = torch.randn((27,2))\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7765dc67-ef3d-4124-8b80-77f95d540ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6,100 because we want 100 layers to our MLP, 6 for 1x2 embeddings of each\n",
    "# of the 3 characters in the 3-length block of context\n",
    "W1 = torch.randn((6,100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5c4e33d0-85df-4688-8ac4-9ab0c4dba91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to do: emb @ W1 + b1\n",
    "# But won't work because trying to multiply a 3x2 component by a 6 component\n",
    "# Need to flatten.\n",
    "# First try: torch.cat([emb[:,i,:] for i in range(0,block_size)])\n",
    "# But there's something easier: torch.cat(torch.unbind(emb,1),1)\n",
    "# ... But actually there's something easier: \n",
    "# If T contains a total of axbxc items, then T.view(a,b,c) turns it into an axbxc tensor\n",
    "# Regardless of what the shape of T was before.\n",
    "# So do:\n",
    "# (-1 equiv to \"whatever's remaining\")\n",
    "h = emb.view(-1,6) @ W1 + b1 # Take care with this adding b1: b1 gets broadcast, but it's right.\n",
    "h = torch.tanh(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d6e0ddc0-7d93-4774-9992-209ecc04b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape\n",
    "# Now create the final layer:\n",
    "W2 = torch.randn((100,27),requires_grad=True)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bc6f8ac6-8616-4ee1-929c-2cac6afc2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build softmax:\n",
    "def softmax(logits):\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392c834-aed4-4403-92f6-cf5dd310116a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665eae9-399b-4e71-a350-a79d5d93d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)\n",
    "print(Y.shape)\n",
    "# Current probabilities for the correct character:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b672f-edc5-48ae-acf5-35570c2d8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent (mostly copied from before):\n",
    "for k in range(0,10000):\n",
    "    # Forward Pass:\n",
    "    logits = h @ W2 + b2\n",
    "    # probs = softmax(logits)\n",
    "    # loss = -probs[torch.arange(32), Y].log().mean()    \n",
    "    loss = F.cross_entropy(logits, Y) # Same computation, but torch does it better\n",
    "    \n",
    "    #Backward Pass:\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "    W2.data += -0.1 * W2.grad\n",
    "    \n",
    "    if k % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cecf3-4411-44f5-bb44-b7a9d03897da",
   "metadata": {},
   "source": [
    "# Makemore Parts 2 and 3: Cleaned Up and Functional\n",
    "## Optimizing Hyperparameters, Normalizing Initialization, BatchNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8a8c464e-a1e6-4326-95e0-d81667c1536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Cleaning up the above: ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e81263-81c2-419a-88b9-92a2c6d98005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d10e25f8-c041-44e8-a9d1-c2f6ec1f477a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "To optimize initialization, activation, etc:\n",
    "He, Zhang, Ren, Sun:\n",
    "https://arxiv.org/pdf/1502.01852\n",
    "(Kaiming Initialization)\n",
    "These four researchers at Microsoft also came up with residual connections!\n",
    "https://arxiv.org/pdf/1512.03385\n",
    "(Discussed more in gpt.ipynb)\n",
    "\n",
    "\n",
    "Details of why initialization of W1 should be what it is:\n",
    "Glorot and Bengio, Page 253 (5 of pdf):\n",
    "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "Long version: \n",
    "/ (block_size * Cdims)**0.5 because:\n",
    "(following Glorot and Bengio): si = zi * Wi + bi, z^(i+1) = f(si) (z^(i+1) is h for us, i is iteration)\n",
    "Assuming initialization is ~mean-free and f'(0) = 1 for activation function f, \n",
    "Var(z1) = Var(f(s0)) ~ Var(f'(s0) * (z0 * W0 + b0)) ~ Var(z0 * W0) = Var(x * W0) (for input info x)\n",
    "= Var(x) * n * Var(W0), since :\n",
    "For 1-dim independent r.v.s:\n",
    "Var(N1 * N2) = E((N1 * N2)^2) - E(N1 * N2)^2 = E(N1^2 * N2^2) - E(N1)E(N2)^2\n",
    "= E(N1^2) * E(N2^2) - E(N1)^2 * E(N2)^2 \n",
    "= (E(N1^2) - E(N1)^2)(E(N2^2) - E(N2)^2) + E(N1)^2 E(N2^2) + E(N1^2)E(N2)^2 - 2 E(N1)^2 E(N2)^2\n",
    "= Var(N1)Var(N2) + E(N1)^2 Var(N2) + E(N2)^2 Var(N1)\n",
    "~mean-free => ~ Var(N1)Var(N2)\n",
    "For axb * bxc random matrices M1, M2:\n",
    "Treat M1, M2, M1*M2 as a*b, b*c, a*c random vectors. \n",
    "Notice the diagonal entries are the variances of each entry of M1, M2\n",
    "And non-diagonal entries are all zero b/c i.i.d., hence no covariance.\n",
    "Then: \n",
    "Var(M1 * M2) = tr(cov(M1*M2,M1*M2)) by definition\n",
    "= sum_(a*c) cov(M1*M2,M1*M2)\\_(jj) since M1*M2 \"is an axc vector\" with an axc * axc autocov matrix\n",
    "= sum\\_(j in a*c) sum_(i in b) Var(M1_ij * M2_ji) by matrix mult. of columns of M2 onto rows of m1\n",
    "= sum\\_(j in a*c) sum_(i in b) Var(M1_ij) * Var(M2_ji) since indepen dent\n",
    "= sum\\_(j in b) sum_(i in a*c) Var(M1_entry) * Var(M2_entry) since identically distributed, interchange sums\n",
    "= sum\\_(i in b) Var(M1) * Var(M2) since a*c total entries in M1*M2\n",
    "= b Var(M1) Var(M2)\n",
    "For us: M1 = x_j = emb, M2 = W0_j = W1, b = n = (block_size * Cdims) so:\n",
    "Var(emb * W1 + b1) = n * Var(emb) * Var(W1).\n",
    "Want Var(x) nVar(W) ~ Var(x), so scale W0 so that nVar(W) = 1 => Var(W) = 1/n => std(W) = sqrt(1/n)\n",
    "=> W should be multiplied by 1 / (block_size * Cdims)**0.5 \n",
    "(since it's normal so std(kW) = k std(W)) \n",
    "If f'(0) = k, then Var(z1) = f'(s0)**2 * Var(s0), so also divide by sqrt(f'(0)).\n",
    "\n",
    "Notice:\n",
    "delta=0.0001\n",
    "print(F.gelu(torch.tensor(delta))/torch.tensor(delta))\n",
    "returns about 0.5, so f'(0) = 0.5, so divide by 0.5**0.5... (e.g. multiply by 2**0.5)\n",
    "\n",
    "(As a side note: nxm matrix means n rows, m columns (so n tall, m wide)). We're in CS world, not math world.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Batch Normalization - Ioffe and Szegedy\n",
    "https://arxiv.org/pdf/1502.03167\n",
    "The earlier initialization process discussed above allows h to be initialized to be standard normal,\n",
    "but this paper made it explicit: Just normalize h after every layer to actually be standard normal\n",
    "\n",
    "Second order effect of batch normalization: Makes all of the learned stuff slightly padded out, so that\n",
    "extreme values don't skew the learned process as strongly; regularization process\n",
    "makes it hard to overfit extreme examples. \n",
    "issue with batches without this: Certain recurring batches can skew the data too far by creating the\n",
    "appearance of association when none exists, just because they were repeatedly sampled in the same batch,\n",
    "and can also lead outlier examples to have large impact on gradient descent\n",
    "\n",
    "But conversely, training these normalization values on just batches *can* still create\n",
    "these issues to some degree, so alternatives have been sought: \n",
    "Layer normalization, instance normalization, group normalization\n",
    "\n",
    "Issue with first implementation of batch normalization: Expects batches as input; doesn't function well\n",
    "on single examples input into model. Output of normalization should depend only on that one input in a\n",
    "deterministic fashion.\n",
    "How to fix? Calculate the population mean and std instead of batch statistics. But it's a pain to do this\n",
    "as a separate step at the end, so estimate the population mean and std by keeping a running tally throughout\n",
    "the training process. \n",
    "\n",
    "Issues with batchnorm expressed:\n",
    "https://arxiv.org/pdf/2105.07576\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Notes on resnet:\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "Convolutional layers: To be used similarly to linear layers, but, of course, they convolve the data.\n",
    "Multiply / convolve data with weight layers...\n",
    "What does resnet do?\n",
    "Convolve 1x1, batchnorm, relu, convolve 3x3, batchnorm, relu, convolve 1x1, batchnorm, relu.\n",
    "Note the bias=False arg in convolution layers because of the subsequent batchnorm. \n",
    "Lastly, they do a residual connection. What's that?\n",
    "\n",
    "Layers:\n",
    "torch.nn.Linear(in_features, out_features, bias=True...):\n",
    "in_features: Size of input sample\n",
    "out_features: Size of output sample\n",
    "\n",
    "torch.nn.BatchNorm1d(num_features, epsilon=0.00001, momentum=0.1, affine=True, track_running_stats=True, ...)\n",
    "momentum = running mean, std coefficient; we use 0.001; if larger batch sizes, might use larger momentum\n",
    "affine = learnable scaling and bias normalization factors; should stay True\n",
    "track_running_stats = lets you disable running stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "c5acc786-0550-47e4-9a53-ffdfb2266bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into three parts: Training, validation, and testing\n",
    "# Validation is for tuning hyperparameters;\n",
    "# Be cautious running too long on validation or testing, else you\n",
    "# fit to the new data too much! \n",
    "\n",
    "def conv(words):\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i:s for s,i in stoi.items()}\n",
    "    \n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = conv(words)\n",
    "\n",
    "\n",
    "def build_dataset(words, block_size):\n",
    "    Xlst, Ylst = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            Xlst.append(context)\n",
    "            Ylst.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(Xlst)\n",
    "    Y = torch.tensor(Ylst)\n",
    "    return X, Y\n",
    "\n",
    "# trn: portion to be training set\n",
    "# devn: portion to be validation set\n",
    "def split_dataset(words, block_size, trn, devn, seed=None):\n",
    "    random.Random(seed).shuffle(words)\n",
    "    n1 = int(trn*len(words))\n",
    "    n2 = int((trn+devn)*len(words))\n",
    "    Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "    Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
    "    Xte, Yte = build_dataset(words[n2:], block_size)\n",
    "    return Xtr, Ytr, Xdev, Ydev, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "0dbc670b-4321-497a-b602-16e7a45011ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(Hyperparameters, g=None):\n",
    "    [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity] = Hyperparameters\n",
    "    C = torch.randn((27,Cdims),generator=g) #<--- Input layer where values are embedded\n",
    "    W1 = torch.randn((Cdims*block_size, layers), generator=g) / (0.5 * block_size * Cdims)**0.5 #<--- Hidden layers of MLP weights\n",
    "    # See above for why / (f'(0), block_size, Cdims)**0.5, f'(0) = Phi(0) = 0.5, hence...\n",
    "\n",
    "    # Commented out b1 because batch normalization removes bias before normalization, then adds it back in:\n",
    "    # b1 = torch.randn(layers, generator=g) * 0.01 #<--- Bias for hidden layers\n",
    "    \n",
    "    W2 = torch.randn((layers,27), generator=g) * 0.01 #<--- Output layer of probabilities\n",
    "    b2 = torch.randn(27, generator=g) * 0 #<--- Bias for output layer\n",
    "\n",
    "    # For batch normalization:\n",
    "    normscale = torch.ones(layers)\n",
    "    normbias = torch.zeros(layers)\n",
    "\n",
    "    normscale_running = torch.ones(layers)\n",
    "    normbias_running = torch.zeros(layers)\n",
    "    \n",
    "    parameters = [C, W1, W2, b2, normscale, normbias]\n",
    "    statistics =  [normscale_running, normbias_running]\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    return parameters, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "898ce846-b7e7-4ae7-a40d-f48654e26647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find best descentspeed / learning rate?\n",
    "# What if you search over them?!\n",
    "# Per video: A good lower bound seems to be .001, upper bound 1\n",
    "# lre = torch.linspace(-3,0,1000)\n",
    "# lrs = 10**lre\n",
    "# lossi = []\n",
    "# lri = []\n",
    "\n",
    "# I didn't really believe Karpathy because I thought that maybe the best\n",
    "# learning rate might depend on how much we've already learned, so I tried\n",
    "# iterating (slowly) over the different learning rates, re-initializing \n",
    "# all of the tensors each time to compare more accurately:\n",
    "# for j in range(0,100):\n",
    "#     C = torch.randn((27,Cdims),generator=g)\n",
    "#     W1 = torch.randn((Cdims*block_size, layers), generator=g)\n",
    "#     b1 = torch.randn(layers, generator=g)\n",
    "#     W2 = torch.randn((layers,27), generator=g)\n",
    "#     b2 = torch.randn(27, generator=g)\n",
    "#     parameters = [C, W1, b1, W2, b2]\n",
    "#     for p in parameters:\n",
    "#         p.requires_grad = True\n",
    "# (result: it didn't make a dramatic difference)\n",
    "# Why doesn't it matter? Because at this stage, trying to just get a rough estimate, and it's so much faster\n",
    "# than re-initializing every time. Consequently, you can make much faster progress and then do the same process\n",
    "# again on a tigher bound to fine-tune initial step size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "id": "05080032-cc17-4e8d-af4b-754451029c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def stochgrad(parameters, statistics, Hyperparameters):\n",
    "    [C, W1, W2, b2, normscale, normbias] = parameters\n",
    "    [normscale_running, normbias_running] = statistics\n",
    "    [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity] = Hyperparameters\n",
    "    lossi = []\n",
    "    lri = []\n",
    "    stepi = []\n",
    "    \n",
    "    sizemod = max(int(math.log10(steps))-3,1)\n",
    "    for j in range(0,sizemod):\n",
    "        lrmult = 10**(-j)\n",
    "        \n",
    "        for k in range(0,steps//sizemod):\n",
    "            # Make mini-batches to iterate over instead:\n",
    "            # (batch_size,) because needs to be a tuple for the size\n",
    "            ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
    "            # C[X] consists of the \"words\" in X embedded into the learned embedding space\n",
    "            # C, which contains embeddings of each character and how they're associated to each other\n",
    "            emb = C[Xtr[ix]] # (batch_size, block_size, Cdims) tensor\n",
    "\n",
    "            # Linear layer:\n",
    "            embcat = emb.view(-1,(block_size * Cdims)) # Concatenate submatrices of C into vectors to multiply\n",
    "            hpreact = embcat @ W1 # Initial value of h, pre-activation\n",
    "\n",
    "            # Batch Normalization:\n",
    "            # -------------------------------------------------------------------------\n",
    "            # Track the normalization values:\n",
    "            normscalei = hpreact.std(0, keepdim=True)\n",
    "            normbiasi = hpreact.mean(0, keepdim=True)\n",
    "\n",
    "            # Normalization layers:\n",
    "            normedh = (hpreact - normbiasi) / (normscalei + 0.00001) # Normalize it, add an epsilon>0 in case no-variance batch occurs\n",
    "            normedh = normscale * normedh + normbias # Modify by learned scaling and bias\n",
    "\n",
    "            with torch.no_grad():\n",
    "                normscale_running = 0.999 * normscale_running + 0.001 * normscalei\n",
    "                normbias_running = 0.999 * normbias_running + 0.001 * normbiasi\n",
    "            # --------------------------------------------------------------------------\n",
    "\n",
    "            \n",
    "            # Activation layers: \n",
    "            h = F.gelu(normedh) # (total combinations, layers) tensor\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Forward Pass:\n",
    "            logits = h @ W2 + b2\n",
    "            \n",
    "            loss = F.cross_entropy(logits, Ytr[ix]) + creativity*(W2**2).mean() * lrmult\n",
    "            #Backward Pass:\n",
    "            for p in parameters:\n",
    "                p.grad = None\n",
    "            loss.backward()\n",
    "                \n",
    "            for p in parameters:\n",
    "                p.data += -descentspeed * p.grad *lrmult\n",
    "            \n",
    "            # Track loss performance\n",
    "            # lri.append(lre[k])\n",
    "            lossi.append(loss.item())\n",
    "            stepi.append(k)\n",
    "            \n",
    "            if k % 10000 == 0:\n",
    "                print(loss.item())\n",
    "            \n",
    "            # Note that stochastic gradient descent doesn't converge as monotonically as \n",
    "            # ordinary gradient descent, because sometimes it sees new batches that throw things off\n",
    "    \n",
    "    parameters = [C, W1, W2, b2, normscale, normbias]\n",
    "    statistics = [normscale_running, normbias_running]\n",
    "    \n",
    "    return parameters, statistics, loss, lossi, stepi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "18d2f9ab-dbe6-4434-895e-06a9a7b71ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check loss:\n",
    "@torch.no_grad() # Disables computing gradients in anticipation of backward pass\n",
    "def checkloss(X, Y, parameters, statistics, Hyperparameters):\n",
    "    [C, W1, W2, b2, normscale, normbias] = parameters\n",
    "    [normscale_running, normbias_running] = statistics\n",
    "    [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity] = Hyperparameters\n",
    "    \n",
    "    emb = C[X] # (total combinations, block_size, Cdims) tensor\n",
    "    \n",
    "    embcat = emb.view(-1,(block_size * Cdims)) # Concatenate submatrices of C into vectors to multiply\n",
    "    hpreact = embcat @ W1 # Initial value of h, pre-activation\n",
    "    \n",
    "    normedh = (hpreact - normbias_running) / (normscale_running + 0.00001) # Normalize it\n",
    "    normedh = normscale * normedh + normbias # Modify by learned scaling and bias\n",
    "    \n",
    "    h = F.gelu(normedh) # (total combinations, layers) tensor\n",
    "    \n",
    "    logits = h @ W2 + b2  \n",
    "    loss = F.cross_entropy(logits, Y) + creativity*(W2**2).mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "38894e7f-b689-486d-b78f-f8f79280233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from the model:\n",
    "\n",
    "def generate(num, parameters, statistics, Hyperparameters, g=None):\n",
    "    [C, W1, W2, b2, normscale, normbias] = parameters\n",
    "    [normscale_running, normbias_running] = statistics\n",
    "    [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity] = Hyperparameters\n",
    "\n",
    "    for k in range(num):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            emb = C[torch.tensor([context])]\n",
    "            \n",
    "            embcat = emb.view(1,-1)\n",
    "            hpreact = embcat @ W1 # Initial value of h, pre-activation\n",
    "            \n",
    "            normedh = (hpreact - normbias_running) / (normscale_running + 0.00001) # Normalize it\n",
    "            normedh = normscale * normedh + normbias # Modify by learned scaling and bias\n",
    "            \n",
    "            h = F.gelu(normedh) # (total combinations, layers) tensor\n",
    "            \n",
    "            logits = h @ W2 + b2\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "            if ix == 0:\n",
    "                break\n",
    "    \n",
    "        print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "924d2b75-0aec-4467-90cc-75a881e33bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()#.manual_seed(2147483647)\n",
    "block_size = 4\n",
    "Cdims = 8\n",
    "layers = 200\n",
    "descentspeed = 0.1 # <--- In practice, want to decay the learning rate in later iterations\n",
    "steps = 300000\n",
    "# stages of training to fine-tune better as loss gets closer to true value\n",
    "batch_size = 64\n",
    "creativity = 0.0001\n",
    "Hyperparameters = [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "4019532c-2530-42ce-9268-e24a7fda710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi, itos = conv(words)\n",
    "Xtr, Ytr, Xdev, Ydev, Xte, Yte = split_dataset(words, block_size, 0.8, 0.1)\n",
    "parameters, statistics = initialize_parameters(Hyperparameters, g=g)\n",
    "[C, W1, W2, b2, normscale, normbias] = parameters\n",
    "[normscale_running, normbias_running] = statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f14ca-ace6-435d-8e9c-467c7a047b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots to see how frequently do extreme values of h occur:\n",
    "# Don't want too much white space, because then activation functions aren't *doing* anything--\n",
    "# A dead neuron can cause issues. \n",
    "\n",
    "# print(W1.shape)\n",
    "# print(emb.view(-1,(block_size * Cdims)).shape)\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
    "emb = C[Xtr[ix]]\n",
    "h = F.gelu(emb.view(-1,(block_size * Cdims)) @ W1 + b1)\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.imshow(h.abs() > 0.99, cmap='gray', interpolation='nearest')\n",
    "\n",
    "print(h.shape)\n",
    "print(h.mean(0, keepdim=True).shape)\n",
    "print(h.std(0, keepdim=True).shape)\n",
    "# Normalize by:\n",
    "h = (h - h.mean(0, keepdim=True))/ h.std(0, keepdim=True) #<--- However, this doesn't produce a desireable outcome!\n",
    "# Why? Because too strong of normalization just centers the input into the activation function too close around 0.\n",
    "# Need a model with the capacity to understand extremal values. So introduce scaling and shifting parameters\n",
    "# that are also learned by the model. Each step of stochastic gradient descent over them produces estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "id": "96ab32fc-2449-49ec-ac95-081542b0964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d81a70-c88e-483c-b383-1fa8b70b534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtr.shape[0]) # <--- Total combinations\n",
    "print(sum(p.nelement() for p in parameters)) # <--- Total number of parametersleft-align cell jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "2c8d8cd1-b18d-40a6-a43f-7106e4170490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Cdims Hyperparam:\n",
    "# for i in range(2,12,1):\n",
    "#     Cdims = i\n",
    "#     Hyperparameters = [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity]\n",
    "#     parameters = initialize_parameters(Hyperparameters, g=g)\n",
    "#     parameters, loss, lossi, stepi = stochgrad(parameters, Hyperparameters)\n",
    "#     print(checkloss(Xtr, Ytr, parameters, Hyperparameters))\n",
    "#     print(checkloss(Xdev, Ydev, parameters, Hyperparameters))\n",
    "# Based on 50k steps for each, it seems like Cdim=8 or 9 were optimal; choosing 8 since simplicity is better.\n",
    "\n",
    "# Same thing but for layers:\n",
    "# for i in range(60,600,20):\n",
    "#     layers = i\n",
    "#     Hyperparameters = [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity]\n",
    "#     parameters = initialize_parameters(Hyperparameters, g=g)\n",
    "#     parameters, loss, lossi, stepi = stochgrad(parameters, Hyperparameters)\n",
    "#     print(i)\n",
    "#     print(checkloss(Xtr, Ytr, parameters, Hyperparameters))\n",
    "#     print(checkloss(Xdev, Ydev, parameters, Hyperparameters))\n",
    "# Hard to make a conclusive decision about layers given the results. \n",
    "# I found 120, 220, and 500 to give the best results relative to their sizes, but their neighbors didn't\n",
    "# necessarily reflect this, so I'm interpreting it to be essentially random after a certain point.\n",
    "# 100 definitely seems like too few, 500 definitely seems like too many. Going with 200 as a safe middle point.\n",
    "\n",
    "# Same thing but for batch sizes relative to time taken and # steps:\n",
    "# for i in range(3,10):\n",
    "#     batch_size = 2**i\n",
    "#     steps = 20000//i\n",
    "\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     Hyperparameters = [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity]\n",
    "#     parameters = initialize_parameters(Hyperparameters, g=g)\n",
    "#     parameters, loss, lossi, stepi = stochgrad(parameters, Hyperparameters)\n",
    "#     print(i)\n",
    "#     print(checkloss(Xtr, Ytr, parameters, Hyperparameters))\n",
    "#     print(checkloss(Xdev, Ydev, parameters, Hyperparameters))\n",
    "#     print(start_time - time.time())\n",
    "# i=6 was the fastest, i=8 produced most accurate results, although the specific accuracy didn't\n",
    "# change very much between i=5 and i=9, while the time doubled with i=8.\n",
    "# Going with i=6 (batch_size = 64) since the time improvements were significant but accuracy\n",
    "# improvements were not\n",
    "\n",
    "# Lastly, same thing but for context window:\n",
    "# for i in range(2,9):\n",
    "#     block_size = i\n",
    "\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     Hyperparameters = [block_size, Cdims, layers, batch_size, descentspeed, steps, creativity]\n",
    "#     Xtr, Ytr, Xdev, Ydev, Xte, Yte = split_dataset(words, block_size, 0.8, 0.1)\n",
    "#     parameters = initialize_parameters(Hyperparameters, g=g)\n",
    "#     parameters, loss, lossi, stepi = stochgrad(parameters, Hyperparameters)\n",
    "#     print(i)\n",
    "#     print(checkloss(Xtr, Ytr, parameters, Hyperparameters))\n",
    "#     print(checkloss(Xdev, Ydev, parameters, Hyperparameters))\n",
    "#     print(time.time() - start_time)\n",
    "# 4 seems to be the best, followed by 6 and then 5. Surprisingly the different context window sizes\n",
    "# didn't seem to make as big of a difference in time taken as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ef851-5ad6-44dd-a029-b2be832b0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, statistics, loss, lossi, stepi = stochgrad(parameters, statistics, Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "063b1551-1eac-40e0-910b-a2ecfcdf1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[C, W1, W2, b2, normscale, normbias] = parameters\n",
    "[normscale_running, normbias_running] = statistics\n",
    "# print(normscale, normbias, normscale_running, normbias_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637d4f3-1a5d-4eae-8620-21f9c357221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())\n",
    "# print(lri, lossi)\n",
    "# plt.plot(lri, lossi)\n",
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec54550-6efc-4a40-aff4-562eaec73ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkloss(Xtr, Ytr, parameters, statistics, Hyperparameters))\n",
    "print(checkloss(Xdev, Ydev, parameters, statistics, Hyperparameters))\n",
    "print(checkloss(Xte, Yte, parameters, statistics, Hyperparameters))\n",
    "\n",
    "# Best values obtained:\n",
    "# Training loss: 1.985\n",
    "# Validation loss: 2.0695\n",
    "# Testing loss: 2.0509\n",
    "# E01 complete! \n",
    "# I didn't see there were exercises in the description of makemore parts 1 and 2\n",
    "# until later, so I've skipped those from part 1 and question 2 from part 2 as they feel less necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c359b-d650-47bb-8a8b-cd6497d2fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(20, parameters, statistics, Hyperparameters, g=g)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf58df2a-3fe2-4e80-a69a-89fd2c689bd7",
   "metadata": {},
   "source": [
    "Funny results from over-training the data:\n",
    "musten.\n",
    "musten.\n",
    "mustlengelosmin.\n",
    "mustlengelassandollemunnestophemumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumaksurlennellemumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumamusrumbelusmumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumaduslumanuslubbenmellemunnestofemuxtemumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumumaduslubegustem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db10bd62-113e-423c-a260-aa17bad1c8ca",
   "metadata": {},
   "source": [
    "# Makemore Parts 3 and 5: Torch-ified and with WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbe8062-7866-4d8b-951d-f772529bcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ Torch-ifying the code: ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cfe54d-0c3d-4426-bdf9-7085b3ab300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85fb5c6-05a7-432f-8397-cd6b71edf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053fc744-9369-4b9a-a983-e8f304adb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that most of the classes below can be prefixed torch.nn. and they work the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68d06fc-fdca-4363-99d7-5067d4354494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# Linear layer as a class; consists of a matrix of weights and a bias vector\n",
    "# Note the normalization of initial weight values depends on activation function used; in this case it's gelu so multiply by 2**0.5,\n",
    "# but that goes much below when initializing the actual MLP:\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.weight = torch.randn((in_features, out_features)) / (in_features)**0.5\n",
    "        self.bias = torch.zeros(out_features) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# BatchNorm layer as a class; consists of a scale normalization parameter and a bias normalization parameter\n",
    "# Tracks changes in scale param and bias param over time.\n",
    "# eps: epsilon value to keep from dividing by 0\n",
    "# momentum: Rate of change of batchnorm params\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        # Parameters:\n",
    "        self.gamma = torch.ones(dim) # normscale\n",
    "        self.beta = torch.zeros(dim) # normbias\n",
    "\n",
    "        # Buffers (running values):\n",
    "        self.running_var = torch.ones(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Forward pass:\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "\n",
    "            xmean = x.mean(dim, keepdim=True)\n",
    "            xvar = x.var(dim, keepdim=True, unbiased=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        normx = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * normx + self.beta\n",
    "\n",
    "        # Update buffers:\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Gelu activation function\n",
    "# As is visible, just calls that actual gelu function in Torch. :)\n",
    "class Gelu:\n",
    "    def __call__(self, x):\n",
    "        self.out = F.gelu(x)\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return []\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# To torch-ify the embedding of the characters into a vector space, we create:\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "        \n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# And to torch-ify the flattening of the embedding (it's convenient to treat it as\n",
    "# its own layer so you can remove it from the forward pass)\n",
    "class FlattenConsecutive:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B,T//self.n,C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        \n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Sequential: A torch *container*. What's a container? It's a thing that's got\n",
    "# more things in it! We can use it to more efficiently contain our entire\n",
    "# neural net, instead of having a list of all layers\n",
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Note these are classes that we *treat like functions* by giving them a self.out. \n",
    "# Unlike a function, though, they're also objects (except Flatten, which doesn't have an init)\n",
    "# So the effect of doing self.out is that it essentially \"runs the function part\" of the object, \n",
    "# storing the output in the object itself\n",
    "# (an instance of conflating a function with its image)\n",
    "\n",
    "# So a layer is an object with data in it, and with parameters that modify the data via some function.\n",
    "# Running the layer applies that function to the data in it, and continues to store the data\n",
    "# The reason why backprop is structured as it is below is to first forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae314e1-daab-4cb7-a678-980991ba7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct copypasta from above:\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Converts characters to numerical form, includes '.' for starts/ends of words\n",
    "def conv(words):\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i:s for s,i in stoi.items()}\n",
    "    \n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = conv(words)\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# As titled, builds numerical dataset with some context window legnth of block_size over all words in the dataset\n",
    "def build_dataset(words, block_size):\n",
    "    Xlst, Ylst = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            Xlst.append(context)\n",
    "            Ylst.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(Xlst)\n",
    "    Y = torch.tensor(Ylst)\n",
    "    return X, Y\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# trn: portion to be training set\n",
    "# devn: portion to be validation set\n",
    "def split_dataset(words, block_size, trn, devn, seed=None):\n",
    "    random.Random(seed).shuffle(words)\n",
    "    n1 = int(trn*len(words))\n",
    "    n2 = int((trn+devn)*len(words))\n",
    "    Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
    "    Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
    "    Xte, Yte = build_dataset(words[n2:], block_size)\n",
    "    return Xtr, Ytr, Xdev, Ydev, Xte, Yte\n",
    "# -------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ccaf140-fc23-4b27-80e1-9323fa00ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def stochgrad(model, Hyperparameters, X, Y):\n",
    "    [block_size, emb_dims, layer_count, batch_size, descentspeed, steps, creativity, vocab_size, g] = Hyperparameters\n",
    "    lossi = []\n",
    "    ud = []\n",
    "\n",
    "    # Trains faster at start, then slows down as more iterations occur w/ sizemod\n",
    "    sizemod = max(int(math.log10(steps))-3,1)\n",
    "    for j in range(0,sizemod):\n",
    "        lrmult = 10**(-j)\n",
    "        \n",
    "        for k in range(0,steps//sizemod):\n",
    "            \n",
    "            # Making minibatches: \n",
    "            ix = torch.randint(0, X.shape[0], (batch_size*(2**j),)) \n",
    "            Xb, Yb = X[ix], Y[ix]\n",
    "\n",
    "            # Forward Pass:\n",
    "            logits = model(Xb)\n",
    "            loss = F.cross_entropy(logits, Yb) + creativity*lrmult\n",
    "\n",
    "            # retain_grad() allows me to see gradients of intermediate steps; good for part 4 with understanding how\n",
    "            # backprop actually flows.\n",
    "            # but not necessary for computation; slows things down, in fact\n",
    "            # for layer in layers:\n",
    "            #     layer.out.retain_grad()\n",
    "\n",
    "            # Backward Pass:\n",
    "            for p in model.parameters():\n",
    "                p.grad = None\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update:\n",
    "            for p in model.parameters():\n",
    "                p.data += -descentspeed * p.grad * lrmult\n",
    "            \n",
    "            # Track loss performance\n",
    "            lossi.append(loss.log10().item())            \n",
    "            if k % 10000 == 0:\n",
    "                print(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ud.append([(descentspeed*lrmult*p.grad.std() / p.data.std()).log().item() for p in parameters])\n",
    "\n",
    "    return loss, lossi, ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "627eb791-23ac-46c8-b426-aa6c66e03084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters block\n",
    "\n",
    "g = torch.Generator()#.manual_seed(2147483647)\n",
    "block_size = 8\n",
    "emb_dims = 8\n",
    "hidden_dim = 196\n",
    "layer_count = 1\n",
    "descentspeed = 1\n",
    "steps = 150000\n",
    "batch_size = 32\n",
    "creativity = 0.0001\n",
    "vocab_size = 27 # 26 letters of alphabet + .\n",
    "Hyperparameters = [block_size, emb_dims, layer_count, batch_size, descentspeed, steps, creativity, vocab_size, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f721c77d-a9fd-4fde-9097-b5ee5c00f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Embedding(vocab_size, emb_dims), \n",
    "          \n",
    "          FlattenConsecutive(2), \n",
    "          Linear(emb_dims*2, hidden_dim, bias=False),\n",
    "          BatchNorm1d(hidden_dim), \n",
    "          Gelu(),\n",
    "          \n",
    "          FlattenConsecutive(2), \n",
    "          Linear(hidden_dim*2, hidden_dim, bias=False),\n",
    "          BatchNorm1d(hidden_dim), \n",
    "          Gelu(),\n",
    "          \n",
    "          FlattenConsecutive(2), \n",
    "          Linear(hidden_dim*2, hidden_dim, bias=False),\n",
    "          BatchNorm1d(hidden_dim), \n",
    "          Gelu(),\n",
    "         \n",
    "          Linear(hidden_dim, vocab_size)]\n",
    "# No activation on the final layer\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c43498-acae-4e29-9c45-9a07db63327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Make the last layer less confident:\n",
    "    layers[-1].weight *= 0.1\n",
    "    \n",
    "    # All other layers: Modify by f'(0) (gain):\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 2**0.5\n",
    "    # Notably: \n",
    "\n",
    "parameters = model.parameters()\n",
    "for p in parameters:\n",
    "    p.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f93552-063b-42d0-a45a-6bd2444c2266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163511\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.nelement() for p in parameters])) # total parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6939c45-6557-44d0-8bc3-e9efb5249297",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Xtr, Ytr, Xdev, Ydev, Xte, Yte] = split_dataset(words, block_size, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd9235-6edc-4ea8-975a-693231ba188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, lossi, ud = stochgrad(model, Hyperparameters, Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df675319-5b95-4845-86e8-dc40a62eb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c03c0-ecf9-4423-ae71-d4b3fd3b0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "# Bad plot:\n",
    "# plt.plot(lossi)\n",
    "\n",
    "# Better plot:\n",
    "plt.plot(torch.tensor(lossi).view(-1,1000).mean(1)) # averages into buckets of 1000 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e70319-a2f2-4728-9a78-10587d6efefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check probability distribution for activation function--will look very different from\n",
    "# tanh because gelu is different.\n",
    "# Want saturation ~5%\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "legends=[]\n",
    "for i, layer in enumerate(layers[:-1]): # exclude output layer\n",
    "    if isinstance(layer, Gelu):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), ((t.abs() > 2)).float().mean()*100))\n",
    "        # ^ slightly modified due to gelu being different from tanh\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a55fe8-0948-42c6-a573-264a66b2f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass activation statistics\n",
    "# Want roughly similar gradients to each other; the last layer looks a little funny but the rest are fine\n",
    "plt.figure(figsize=(20,4))\n",
    "legends=[]\n",
    "for i, p in enumerate(parameters):\n",
    "    t = p.grad\n",
    "    if p.ndim == 2:\n",
    "        print('weight %10s | mean %+f | std %e, | grad:data ratio: %e' % (tuple(p.shape), t.mean(), t.std(), t.std()/p.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'{i} ({tuple(p.shape)}')\n",
    "plt.legend(legends);\n",
    "plt.title('weights gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7b97f-013a-431b-8305-71c15988260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training rate--note it should shrink as the training rate shrinks as intended above\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "legends=[]\n",
    "for i, p in enumerate(parameters): # exclude output layer\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "        legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # Ratios should be ~1e-3, heuristically\n",
    "plt.legend(legends);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0896d-7207-4cc8-8c6b-c77ab84b17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma means should be around 1, betas around 0, std shouldn't be too large\n",
    "# Running might have larger std though\n",
    "for i, layer in enumerate(layers[:-1]): # exclude output layer\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        print('layer %d (%10s): Gamma: mean %+.2f, std %.2f, Beta: mean %.2f, std %.2f, Running: mean %.2f, std %.2f' % (i, layer.__class__.__name__, layer.gamma.mean(), layer.gamma.std(), layer.beta.mean(), layer.beta.std(), layer.running_mean.mean(), layer.running_var.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebc2e07f-8fd1-4df2-a329-1560313d9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Disables computing gradients in anticipation of backward pass\n",
    "def checkloss(X, Y, model):    \n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, Y) + creativity\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8bb864a-8dbf-4d57-a08c-ea5cdf387312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from the model:\n",
    "\n",
    "def generate(num, model, g=None):\n",
    "    for k in range(num):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            logits = model(torch.tensor([context]))\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "            if ix == 0:\n",
    "                break\n",
    "    \n",
    "        print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50d560b8-b611-4192-a042-75f8a3c74d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1e218-75bb-4bab-b660-990681804cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkloss(Xdev, Ydev, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b374ac-6273-4561-8b16-067f6ac4036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(20, model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b39b33a6-85ee-492d-b58e-df3234abd731",
   "metadata": {},
   "source": [
    "Exercises from part 3:\n",
    "\n",
    "- E01: \"I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\"\n",
    "\n",
    "#self.weight = torch.zeros(in_features, out_features)\n",
    "\n",
    "I did this, and only the last layer had nonzero grad! This was the same when I swapped out for tanh. In terms of the layers themselves: The first and the last took on interesting values / trained; everything else was all either ones or zeros. \n",
    "\n",
    "- E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n",
    "\n",
    "Done, same result. Question: Does it significantly improve efficiency to toss out normalization layers after training is done?\n",
    "For batchnorm it doesn't seem to matter that much... what about more modern kinds of normalization?\n",
    "\n",
    "Answer:\n",
    "Want to do this when you're actively changing weights to stop weights from exploding, but they're unnecessary otherwise. Anything that makes anything a little faster is big at the level of LLMs--even if it only makes a .1% improvement. Standard normalization depends on model. It could make a difference. More of an architectural choice to decide which normalization to use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b88f61-43d5-4ccd-8448-fa2d25cb3474",
   "metadata": {},
   "source": [
    "# Part 4: Backprop Ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37343f-78ce-4f18-b9d7-07eef05d4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "44db9577-0e66-4052-8d6c-93fbda53d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fd607-7fd4-4040-9cfc-2fba624e30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f1f83d28-f333-4037-8566-b99fd56bd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9279c67-2091-4b83-aa8e-c03d2916fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "loss.retain_grad()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f104636-7900-4b36-a1e3-6a9f33ace908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# LOSS\n",
    "# How fast does loss change wrt itself? Well, its partial must be 1, because\n",
    "# a nudge in itself produces an identical nudge in itself.\n",
    "dloss = torch.tensor(1.0)\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# LOGPROBS\n",
    "# Refresher (for myself) on what we're doing:\n",
    "# loss = f(logprobs) for some function f.\n",
    "# know dloss/dloss = 1\n",
    "# want dloss/dlogprobs\n",
    "# note: loss = f(logprobs)\n",
    "# f(x) = -x.mean() = -sum(x) / (32*27)\n",
    "# f: R^(32x27) -> R\n",
    "# df: R^(32x27) -> R^(32x27) with entries del z / del (xi,yj) = -1/32*27\n",
    "# since we do 1/ df, we should have -32*27 in each entry\n",
    "# 32x27 is col x row notation\n",
    "# print(logprobs.shape)\n",
    "# print(Yb.shape)\n",
    "# print(range(n))a\n",
    "# print(logprobs[range(n),Yb])\n",
    "# print(logprobs[range(n),Yb].shape)\n",
    "\n",
    "# Really, we should break down f into smaller pieces than this:\n",
    "# h = logprobs[range(n),Yb]: R^32x27 -> R^32, outputting in the kth row the Yb[k]th row entry\n",
    "# g = -mean(logprobs[range(n),Yb]): R^32 -> R, outputting the mean of all entries\n",
    "# dh(logprobs)/dlogprobs: TR^32x27 -> TR^32 is a 32x 32x27 tensor: T. T(i,j,k) = 1 if Yb(j) = k else 0\n",
    "# since logprobs[range(n),Yb] outputs the kth column entry for each of the j rows.\n",
    "# dg(h(logprobs))/dh(logprobs): TR^32 -> TR is a 1x 32 tensor: T'. T'(i) = -1/32, since we're taking the mean of 32 non-zero entries\n",
    "# Now, chain rule: dloss/dlogprobs = df(logprobs)/dlogprobs = d(g o h(logprobs))/dlogprobs\n",
    "# = dg o h(logprobs)/dh(logprobs) * dh(logprobs)/dlogprobs: TR^32x27 -> TR is a 1x 32x27 tensor T''.\n",
    "# T''(j,k) = -1/32 if Yb(j) = k else 0\n",
    "# (It helps (me) to recall that the gradient is dual to the total derivative:\n",
    "# Given (p,v) in TR^n, <grad f(p), v> = delf/delv |_p = df_p(v))\n",
    "dlogprobs = (-1.0/n) * torch.ones_like(logprobs) * (torch.arange(27).unsqueeze(0) == Yb.unsqueeze(1))\n",
    "\n",
    "# Karpathy has a much better way of writing it: \n",
    "# dlogprobs = torch.zeros_like(logprobs)\n",
    "# dlogprobs[range(n),Yb] = -1.0/n\n",
    "\n",
    "# print(dlogprobs)\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# PROBS\n",
    "# dloss / dprobs = dloss / dlogprobs * dlogprobs / dprobs\n",
    "# dlogprobs / dprobs = dlog(probs)/dprobs = 1/probs\n",
    "dprobs = 1/probs * dlogprobs\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# COUNTS_SUM_INV\n",
    "# dloss / dcounts_sum_inv = dloss / dlogprobs * dlogprobs / dprobs * dprobs / dcounts_sum_inv\n",
    "# probs = counts * counts_sum_inv\n",
    "# dprob/dcounts_sum_inv = counts \n",
    "# dcounts_sum_inv = dprobs * counts? Why no work? Hmmm...\n",
    "# Well, in 1D, if f(x) = cx, then df/dx = c. But now f: R^(32x 32x27) -> R^(32x27) is given by f(x,y) = y*x\n",
    "# ...\n",
    "# Watched a bit of the Karpathy video to figure it out. Damned broadcasting! \n",
    "# Convenient, sure, but doesn't make my math brain happy.\n",
    "# Ok, so we want to turn counts_sum_inv: 32x1 into counts_sum_inv: 32x27:\n",
    "# counts * counts_sum_inv = counts * counts_sum_inv.unsqueeze(1).expand(32, 27)\n",
    "# which means for each row, we sum all of the values in counts of that row together\n",
    "# meaning the derivative is equal to the sum of those values:\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "# (had to watch the Karpathy video to get the keepdim=True part...)\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# COUNTS_SUM\n",
    "# dcounts_sum_inv/dcounts_sum = df/dx f(x) = 1/x, df = -1/x^2\n",
    "dcounts_sum = -1/(torch.square(counts_sum)) * dcounts_sum_inv\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# COUNTS\n",
    "# dcounts_sum_inv/dcounts = dcounts_sum_inv/dcounts_sum * dcounts_sum/dcounts\n",
    "# Initialize each column of dcounts as just ones times \n",
    "# probs = counts * counts_sum_inv\n",
    "# dprob/dcounts = counts * dcounts_sum_inv/dcounts + dcounts * counts_sum_inv\n",
    "dcounts = torch.ones_like(counts) * dcounts_sum + counts_sum_inv*dprobs # do .ones because dcounts/dcounts = 1\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# NORM_LOGITS\n",
    "dnorm_logits = counts * dcounts\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# LOGIT_MAXES\n",
    "# norm_logits = logits - logit_maxes\n",
    "# reminder of what we're doing:\n",
    "# want: dloss/dlogit_maxes.\n",
    "# have: dloss/dnorm_logits\n",
    "# dloss/dlogit_maxes = dloss/dnorm_logits * dnorm_logits/dlogit_maxes\n",
    "# dnorm_logits/dlogit_maxes = -1\n",
    "# More broadcasting bullshit. norm_logits 32x27 = logits 32x27 - logit_maxes 32x1\n",
    "# is really norm_logits = logits - logit_maxes.expand(32,27)\n",
    "# so it's really:\n",
    "dlogit_maxes = -dnorm_logits.sum(1,keepdim=True)\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# LOGITS\n",
    "# logits_maxes = logits.max(1,keepdim=True).values\n",
    "# dloss/dlogits = dloss/dnorm_logits * dnorm_logits/dlogits\n",
    "# dnorm_logits/dlogits = d(logits - logit_maxes)/dlogits = dlogits/dlogits - dlogit_maxes.expand(32,27)/dlogits\n",
    "# dlogit_maxes/dlogits: TR^32x27 -> TR^32x27, should be = 1 if max value in row else 0\n",
    "dlogits = dnorm_logits + dlogit_maxes*(logits == logit_maxes)\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# h, W2, b2\n",
    "# Woohoo! Made it thru cross-entropy / softmax! \n",
    "# print(h.shape)\n",
    "# print(logits.shape)\n",
    "# print(W2.shape)\n",
    "# print(b2.shape)\n",
    "# ah, so I think here, @ is just regular matrix multiplication\n",
    "# dloss/dh = dloss/dlogits * dlogits/dh\n",
    "# logits = h @ W2 + b2\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "# print(dh.shape)\n",
    "# print(dW2.shape)\n",
    "# print(db2.shape)\n",
    "# Important question: What is this layer? This is the output layer prior to softmaxing\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# HPREACT\n",
    "# dloss/dhpreact = dloss/dh * dh/dhpreact\n",
    "#dh/dhpreact = dtanh(hpreact)/dhpreact\n",
    "# dtanh(x) = d/dx (e^x - e^-x)* 1/(e^x + e^-x) = (e^x + e^-x) * 1/(e^x + e^-x) + (e^x - e^-x) * -(e^x - e^-x)/(e^x + e^-x)^2 \n",
    "# = 1 - ((e^x - e^-x)/(e^x + e^-x))^2 = 1 - tanh(x)^2\n",
    "dhpreact = dh * (1 - torch.tanh(hpreact)**2)\n",
    "# oops, Karpathy writes it better:\n",
    "# dhpreact = (1 - h**2) * dh\n",
    "# for some reason, I get an approximate answer instead of an exact one, despite being identical to Karpathy...\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# BN\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "# dloss/dbngain = dloss/dhpreact * dhpreact/dbngain\n",
    "# print(hpreact.shape)\n",
    "# print(bngain.expand(64,64).shape)\n",
    "# print(bnraw.shape)\n",
    "# print(bnbias.expand(32,64).shape)\n",
    "# bngain is broadcast: 1,64 -> 64x64, bnbias is broadcast: 1,64 -> 32x64\n",
    "# hpreact = bnraw @ bngain.expand(64,64) + bnbias.expand(32,64)\n",
    "dbngain = (dhpreact * bnraw).sum(0,keepdim=True)\n",
    "#dloss/dbnbias = dloss/dhpreact * dhpreact/dbnbias\n",
    "dbnbias = dhpreact.sum(0,keepdim=True)\n",
    "#dloss/dbnbias = dloss/dhpreact * dhpreact/dbnraw\n",
    "dbnraw = dhpreact * bngain\n",
    "\n",
    "\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# dloss/dbnvar_inv = dloss/dbnraw * dbnraw/dbnvar_inv\n",
    "# dloss/dbndiff = dloss/dbnraw * dbnraw/dbndiff\n",
    "# print(bndiff.shape)\n",
    "# print(bnvar_inv.shape)\n",
    "# print(bnraw.shape)\n",
    "# bnvar_inv is broadcast: 1,64 -> 64x64\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0,keepdim=True)\n",
    "# dbndiff = dbnraw * bnvar_inv\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# d/dbnvar bnvar_inv = -0.5*(bnvar + 1e-5)**-1.5\n",
    "dbnvar = dbnvar_inv * -0.5*(bnvar + 1e-5)**-1.5\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "dbndiff2 = 1/(n-1) * dbnvar.expand(dbndiff2.shape)\n",
    "# bndiff2 = bndiff**2\n",
    "#dloss/dbndiff = dloss/dbnraw * dbnraw/dbndiff\n",
    "# dbnraw/dbndiff = bnvar_inv + bndiff*dbnvar_inv/dbndiff\n",
    "dbndiff = dbnraw * bnvar_inv + 2*bndiff*dbndiff2\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# PRE-BN\n",
    "# bndiff = hprebn - bnmeani\n",
    "# print(bndiff.shape)\n",
    "# print(bnmeani.shape)\n",
    "# print(hprebn.shape)\n",
    "# bnmeani is broadcast: 1,64 -> 32,64\n",
    "# bndiff = hprebn - bnmeani\n",
    "#\n",
    "# dhprebn = dbndiff\n",
    "dbnmeani = -dbndiff.sum(0,keepdim=True)\n",
    "# dloss/dhprebn = dloss/dbndiff * dbndiff/dhprebn = dloss/dbndiff * (dhprebn/dhprebn - dbnmeani/dhprebn)\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# dbnmeani/dhprebn: 1/n*dbnmeani.expand(32,64)\n",
    "dhprebn = dbndiff * torch.ones_like(dhprebn) + 1/n*dbnmeani.expand(dhprebn.shape)\n",
    "# -----------------\n",
    "\n",
    "# -----------------\n",
    "# EMBEDDING\n",
    "# emb = C[Xb] # embed the characters into vectors\n",
    "# embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# # Linear layer 1\n",
    "# hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "print(hprebn.shape)\n",
    "print(embcat.shape)\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "# b1 is broadcast: 64 -> 1,64 -> 32,64\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "# Now wtf is embcat again?\n",
    "print(emb.shape)\n",
    "# embcat takes a 32x3x10 tensor as input and outputs a 32x30 tensor: emb(i,j,k) -> emb(i,j*10+k)\n",
    "# so the backprob should essentially be 1-to-1, just un-viewed appropriately: embcat(i,j) -> emb(i,j//10,j%10)\n",
    "demb = dembcat.view(emb.shape[0],emb.shape[1],emb.shape[2])\n",
    "\n",
    "# What is C again? It's the actual space of learned embeddings of the 27 characters into a 10-dim space.\n",
    "print(C.shape)\n",
    "print(Xb.shape)\n",
    "# so emb: (27,10) x (32,3) -> (32,3,10)\n",
    "# by taking emb(i,j,k) = C(Xb(i,j),k) for a character Xb(i,j) equal to a value between 0 and 26\n",
    "# so demb(i,j,k) = dC(Xb(i,j),k)\n",
    "# Dude I was so upset that I couldn't figure out how to do this with clever broadcasting,\n",
    "# then I watched Karpathy and saw he just used for loops. Ok thanks!\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(0,Xb.shape[0]):\n",
    "    for j in range(0,Xb.shape[1]):\n",
    "        dC[Xb[i,j]] += demb[i,j]\n",
    "# -----------------\n",
    "\n",
    "\n",
    "\n",
    "# Adding one extra line: Change in loss wrt itself is 1\n",
    "cmp('loss', dloss, loss)\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4656a59e-3930-4805-bac0-c4d47faf0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises 2,3: Meh I feel like I don't have to do them it's just simplifying the math expressions;\n",
    "# I'll shamelessly disobey Karpathy's instructions and copy his simplified expressions. \n",
    "# Fuck that shit! I've broken the complicated thing down into simple pieces. I get it. Anything else is just being\n",
    "# messier about it.\n",
    "# Nvm I see Karpathy's simplifications. That's nice, but still hiding the complexity behind a different complicated function!\n",
    "# Meh. I'm fine with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab665b-f85b-4fef-ac29-506d87edb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "    dlogits = F.softmax(logits,1)\n",
    "    dlogits[range(n),Yb] -= 1\n",
    "    dlogits /= n\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    dhpreact = dh * (1 - torch.tanh(hpreact)**2)\n",
    "\n",
    "    dbngain = (bnraw * dhpreact).sum(0,keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0,keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    \n",
    "    dembcat = dhprebn @ W1.TdW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    demb = dembcat.view(emb.shape[0],emb.shape[1],emb.shape[2])\n",
    "    dC = torch.zeros_like(C)\n",
    "    for i in range(0,Xb.shape[0]):\n",
    "        for j in range(0,Xb.shape[1]):\n",
    "            dC[Xb[i,j]] += demb[i,j]\n",
    "\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myanacondaenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
